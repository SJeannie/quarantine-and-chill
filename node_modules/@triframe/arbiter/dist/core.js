"use strict";

Object.defineProperty(exports, "__esModule", {
  value: true
});
exports.updateResource = exports.createCache = exports.upstreamMerge = exports.rollbackPatches = exports.mergeBatch = exports.createBatch = exports.mergePatches = exports.stageNewPatches = exports.unstageLatentMutations = exports.stageLatentMutations = undefined;

var _jsonpatch = require("./jsonpatch");

var _SyncEvent = require("./SyncEvent");

var _core = require("@triframe/core");

_core.Resource.events.on('initialized', initializeResource);

function initializeResource(resource) {
  subscribeToChildEvents(resource);
  let timer;
  resource.on('Δ.change', patches => {
    resource.emit('Δ.changed');
    if (patches.every(patch => patch.isTemporary)) return;
    if (timer) clearTimeout(timer);
    timer = setTimeout(() => {
      if (resource.uid) resource.emit('Δ.sync', new _SyncEvent.SyncEvent({
        resource
      }));
    }, resource['[[syncRate]]']);
  });
  resource.on('Δ.sync', ({
    socket,
    patches,
    batchId,
    resource: what
  }) => {
    // On redis sync, do the same shit. socket can be null, since it wont be attached here?
    if (global[resource.uid]) global[resource.uid].forEach(({
      socket: otherSocket,
      resource
    }) => {
      if (socket != otherSocket) {
        try {
          mergePatches(resource, patches);
          otherSocket.emit(`${resource.uid}.mergePatches`, patches);
        } catch (err) {// ?
        }
      }
    });
    if (socket) socket.emit(`${resource.uid}.mergeBatch`, batchId);
  });

  if (latentMutations[resource.uid]) {
    _jsonpatch.jsonpatch.applyPatch(resource['[[attributes]]'], latentMutations[resource.uid]);
  }
}

const latentMutations = {};

const stageLatentMutations = exports.stageLatentMutations = function (resource, patches) {
  latentMutations[resource.uid] = latentMutations[resource.uid] || [];
  latentMutations[resource.uid].push(...patches);
};

const unstageLatentMutations = exports.unstageLatentMutations = function (resource, patches) {
  latentMutations[resource.uid] = latentMutations[resource.uid].filter(patch => !patches.includes(patch));
};

const stageNewPatches = exports.stageNewPatches = resource => {
  if (!resource['[[patches]]']) return [];
  let stagedPatches = [];
  resource['[[patches]]'].forEach(patch => {
    let isNew = !patch.staged;
    if (!patch.batchId) patch.batchId = resource['[[batch]]'];

    if (isNew) {
      patch.staged = true;
      stagedPatches.push(patch);
    }
  });
  return stagedPatches;
};

const mergePatches = exports.mergePatches = (resource, patches) => {
  resource['[[patches]]'] = _jsonpatch.JSONPatchOT.transform(resource['[[patches]]'], patches);
  return applyPatchesToBase(resource, patches);
};

const createBatch = exports.createBatch = resource => {
  return resource['[[batch]]']++;
};

const mergeBatch = exports.mergeBatch = (resource, batchId) => {
  let patches = resource['[[patches]]'].filter(patch => patch.batchId === batchId);
  resource['[[patches]]'] = resource['[[patches]]'].filter(patch => patch.batchId !== batchId);
  return applyPatchesToBase(resource, patches);
};

const rollbackPatches = exports.rollbackPatches = (resource, patches) => {
  let hashes = JSON.stringify(patches);
  resource['[[patches]]'] = resource['[[patches]]'].filter(patch => !hashes.includes(JSON.stringify(patch)));

  let attributes = _jsonpatch.jsonpatch.deepClone(resource['[[base]]']);

  _jsonpatch.jsonpatch.applyPatch(attributes, resource['[[patches]]']);

  resource['[[attributes]]'] = attributes;
  subscribeToChildEvents(resource);
};

const appendPatches = (resource, patches) => {
  if (!resource || !resource["[[patches]]"]) return [];

  _jsonpatch.jsonpatch.applyPatch(resource['[[attributes]]'], patches);

  resource["[[patches]]"].push(...patches);
  return patches;
};

const applyPatchesToBase = (resource, patches) => {
  _jsonpatch.jsonpatch.applyPatch(resource['[[base]]'], patches);

  let attributes = _jsonpatch.jsonpatch.deepClone(resource['[[base]]']);

  _jsonpatch.jsonpatch.applyPatch(attributes, resource['[[patches]]']);

  resource['[[attributes]]'] = attributes;
  subscribeToChildEvents(resource);
  resource.emit('Δ.rebase', resource['[[base]]']);
  return patches;
};

const upstreamMerge = exports.upstreamMerge = (resource, resource2) => {
  let attributes = _jsonpatch.jsonpatch.deepMerge(resource['[[attributes]]'], resource2['[[attributes]]']);

  let base = _jsonpatch.jsonpatch.deepMerge(resource['[[base]]'], resource2['[[base]]']);

  _jsonpatch.jsonpatch.applyPatch(attributes, resource['[[patches]]']);

  resource['[[base]]'] = base;
  resource['[[attributes]]'] = attributes;
  subscribeToChildEvents(resource);
  resource.emit('Δ.rebase', resource['[[base]]']);
}; // Something about Lists are getting lost after an upstream update
// Must have to do with Object.assign in unserializer and or subscribeToChildEvents; the stream get's broken?


const subscribeToChildEvents = (resource, namespace = false) => {
  return;
  (0, _core.each)(resource['[[attributes]]'], (key, value) => {
    if (value && value.on) {
      value.on('Δ.rebase', base => {
        let target = resource['[[base]]'];
        if (namespace !== false) target = target[namespace];
        target[key] = base;
      });
      value.on('Δ.change', patches => {
        const mappedPatches = patches.map(patch => ({
          op: patch.op,
          path: `/${key}${patch.path}`,
          value: patch.value
        }));
        resource["[[patches]]"].push(...mappedPatches);
        resource.emit('Δ.change', mappedPatches);
      });
    }
  });
};

const global = {};

const createCache = exports.createCache = socket => {
  const bin = {};
  const {
    session
  } = socket;

  const cache = resource => {
    if (bin[resource.uid]) {
      upstreamMerge(bin[resource.uid], resource);
      return;
    }

    global[resource.uid] = global[resource.uid] || [];
    global[resource.uid].push({
      resource,
      socket
    });
    bin[resource.uid] = resource;
    socket.on(`${resource.uid}.sync`, async ({
      batchId,
      patches,
      attributes
    }, respond) => {
      const {
        updateSuccessful,
        invalidPatches
      } = await updateResource(resource, patches, session);
      respond({
        updateSuccessful,
        invalidPatches
      });
      resource.emit('Δ.sync', new _SyncEvent.SyncEvent({
        resource,
        batchId,
        socket
      }));
    });
  };

  const getCached = async uid => {
    return bin[uid];
  };

  return {
    cache,
    getCached
  };
};

const updateResource = exports.updateResource = async (resource, patches, session) => {
  const validPatches = [];
  const invalidPatches = [];
  await (0, _core.eachAsync)(patches, async (index, patch) => {
    const [key] = patch.path.split('/').slice(1);
    const {
      writeAccessTest,
      namespace
    } = (0, _core.getMetadata)(resource, key);

    if (writeAccessTest === undefined) {
      validPatches.push(patch);
    } else {
      let hasAccess = await writeAccessTest.call(resource, {
        session,
        resource
      });
      if (hasAccess) validPatches.push(patch);else invalidPatches.push(patch);
    }
  });
  appendPatches(resource, validPatches);
  let updateSuccessful = validPatches.length === patches.length;
  return {
    updateSuccessful,
    validPatches,
    invalidPatches
  };
};
//# sourceMappingURL=core.js.map